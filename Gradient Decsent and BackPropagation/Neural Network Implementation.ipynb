{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    \n",
    "    def __init__(self,input_shape, output_shape, hidden_shape, n_layers, alpha= 0.001):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.n_layers = n_layers\n",
    "        self.w_list = self.Compile(self.input_shape, self.output_shape, self.hidden_shape, self.n_layers)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "    def Compile(self, input_shape, output_shape, hidden_shape, n_layers ):\n",
    "        w_list, b_list = self.init_w_b_matrices(input_shape, output_shape, hidden_shape)\n",
    "        w_list = self.put_bias_in_w_matrix(w_list, b_list)\n",
    "        return w_list\n",
    "    \n",
    "    \n",
    "    def predict(self,x):\n",
    "        return self.feedforward(self.w_list,x)[0]\n",
    "        \n",
    "        \n",
    "    def feedforward(self,w_list, x):\n",
    "        hidden_values = []\n",
    "        hidden_raw_values = []  # It is actually the summed values of neurons (z)\n",
    "\n",
    "        hidden_values.append(x)\n",
    "        hidden_raw_values.append(x)\n",
    "        \n",
    "        pre_hidden = self.one_adder(x)\n",
    "        for i in range(len(hidden_shape)):\n",
    "            raw = w_list[i] @ pre_hidden\n",
    "            hidden_raw_values.append(raw)\n",
    "            next_hidden =self.sigmoid(raw)\n",
    "            hidden_values.append(next_hidden)\n",
    "            pre_hidden = self.one_adder(next_hidden)\n",
    "            \n",
    "        output_value = w_list[-1] @ self.one_adder(hidden_values[-1])  #note that the activation of the output layer is linear\n",
    "        hidden_values.append(output_value)\n",
    "        hidden_raw_values.append(output_value)\n",
    "        return output_value, hidden_values, hidden_raw_values\n",
    "            \n",
    "            \n",
    "    def init_w_b_matrices(self,input_shape, output_shape, hidden_shape):\n",
    "        w_list = []\n",
    "        w_shapes = []\n",
    "        b_list = []\n",
    "        w_shapes.append((hidden_shape[0],input_shape))\n",
    "        for i in range(len(hidden_shape)-1):\n",
    "            w_shapes.append((hidden_shape[i+1], hidden_shape[i]))\n",
    "        w_shapes.append((output_shape,hidden_shape[-1]))\n",
    "        for i in range(len(w_shapes)):\n",
    "            w = np.random.random((w_shapes[i][0], w_shapes[i][1]))\n",
    "            w_list.append(w)\n",
    "\n",
    "        for i in range(len(w_shapes)):\n",
    "            b_list.append( np.random.random(w_shapes[i][0]))\n",
    "        return w_list, b_list\n",
    "        \n",
    "    def activate(self, x):\n",
    "        x[0] = 1\n",
    "        x[1:] = sigmoid(x[1:])\n",
    "        return x\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+ np.exp(-x))\n",
    "    \n",
    "    def MSE(self, y, y_pred):\n",
    "        error = ((y - y_pred) **2).sum()\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def put_bias_in_w_matrix(self,w_list, b_list):\n",
    "        for i in range(len(w_list)):\n",
    "            b_list[i] = b_list[i].reshape((w_list[i].shape[0],1)) \n",
    "            w_list[i] = np.hstack((b_list[i],w_list[i])) \n",
    "\n",
    "        return w_list\n",
    "    \n",
    "        \n",
    "    def one_adder(self, x):\n",
    "        return np.array([1] + x.tolist())\n",
    "    \n",
    "    \n",
    "    \n",
    "    def w_gradient_calculator(self,all_layers_delta, outputs, n_layers):\n",
    "        w_gradients = [1 for i in range(n_layers-1)]\n",
    "        for i in range(n_layers-1):\n",
    "            w_gradients[i] = self.calculate_grad(all_layers_delta[i], outputs[i])\n",
    "\n",
    "        return w_gradients\n",
    "\n",
    "\n",
    "\n",
    "    def update_weights(self,w_gradients, n_layers):\n",
    "        for i in range(n_layers-1):\n",
    "            self.w_list[i] = self.w_list[i] - self.alpha*w_gradients[i]\n",
    "\n",
    "        return self.w_list\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self,X,Y, epochs=1):\n",
    "        \n",
    "        n_data = len(X)\n",
    "        \n",
    "        for epo in range(epochs):\n",
    "\n",
    "            for i in range(n_data):\n",
    "                x = X[i]\n",
    "                y = Y[i]\n",
    "                results = self.feedforward(self.w_list, x)\n",
    "                predict = result[0]\n",
    "                outputs = result[1]\n",
    "                inputs = result[2]\n",
    "\n",
    "                all_layers_delta = self.all_layers_delta_calculator(predict, y, self.w_list, inputs, self.n_layers)\n",
    "                w_grad = self.w_gradient_calculator(all_layers_delta, outputs, self.n_layers)\n",
    "                self.w_list = self.update_weights(w_grad, n_layers)\n",
    "\n",
    "\n",
    "                \n",
    "    def delta_current_layer_calculator(self, w_next_layer, delta_next_layer, input_current_layer):\n",
    "        # this function will calculate the delat of current layer\n",
    "        #    pre_layer    current_layer    next_layer\n",
    "\n",
    "        w_next_layer = np.delete(w_next_layer,0, axis=1)\n",
    "        activation_prime = self.calculate_activation_prime(input_current_layer).reshape((-1,1))\n",
    "        delta = (w_next_layer.T @ delta_next_layer.reshape((-1,1))) * activation_prime\n",
    "\n",
    "        return delta.reshape((-1))\n",
    "    \n",
    "    \n",
    "    def calculate_grad(self, delta_layer, output_pre_layer):\n",
    "        grad = delta_layer.reshape((-1,1)) @ self.one_adder(output_pre_layer).reshape((1,-1))\n",
    "        return grad\n",
    "    \n",
    "    def last_layer_delta(self,predict, y):\n",
    "        return predict - y\n",
    "    \n",
    "    \n",
    "    def calculate_activation_prime(self,input_of_the_layer):\n",
    "        # the activation is considered to be sigmoid\n",
    "        # if you are using other activation funciton change the derivative in this sectoin\n",
    "        # Note that the derivative of the sigmoid function is sig(x)*(1-sig(x))\n",
    "        derivative = self.sig_derivative(input_of_the_layer)\n",
    "        #derivative = zero_adder(derivative)\n",
    "        return derivative\n",
    "    \n",
    "    \n",
    "    \n",
    "    def zero_adder(self,x):\n",
    "        return np.array([0] + x.tolist())\n",
    "    \n",
    "    \n",
    "    def all_layers_delta_calculator(self,predict, y, w_list,layers_input, n_layers):\n",
    "        n_list = np.array([i for i in range(n_layers)]) # 0,1,2,3,4\n",
    "        n_list_descending = n_list[::-1] # 4,3,2,1,0\n",
    "\n",
    "        all_layers_delta_list = [1 for i in range(n_layers-1)] # [1,1,1,1,1]\n",
    "        all_layers_delta_list[-1] = self.last_layer_delta(predict,y) \n",
    "\n",
    "\n",
    "\n",
    "        for i in n_list_descending[1:-1]: # 3,2,1,0 \n",
    "\n",
    "            w_next_layer = w_list[i]\n",
    "            delta_next_layer = all_layers_delta_list[i - (n_layers-1)]  # -1,-2,-3,-4\n",
    "            input_current_layer = layers_input[i - n_layers]  # -2,-3,-4,-5\n",
    "            delta = self.delta_current_layer_calculator(w_next_layer, delta_next_layer, input_current_layer)\n",
    "            all_layers_delta_list[i - n_layers] = delta\n",
    "\n",
    "        return all_layers_delta_list\n",
    "    \n",
    "    \n",
    "    def sig_derivative(self,x):\n",
    "        return x*(1-x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([2,3,3,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Neural_Network(input_shape, output_shape, hidden_shape, n_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x],[y], epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.65799657, 2.58158639, 1.03691455, 1.2186281 ])"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_w_b_matrices(input_shape, output_shape, hidden_shape):\n",
    "    w_list = []\n",
    "    w_shapes = []\n",
    "    b_list = []\n",
    "\n",
    "    w_shapes.append((hidden_layers[0],input_shape))\n",
    "    for i in range(len(hidden_shape)-1):\n",
    "        w_shapes.append((hidden_shape[i+1], hidden_shape[i]))\n",
    "    w_shapes.append((output_shape,hidden_shape[-1]))\n",
    "\n",
    "    for i in range(len(w_shapes)):\n",
    "        w = np.random.random((w_shapes[i][0], w_shapes[i][1]))\n",
    "        w_list.append(w)\n",
    "\n",
    "    #self.w_shapes = w_shapes\n",
    "\n",
    "\n",
    "    for i in range(len(w_shapes)):\n",
    "        b_list.append( np.random.random(w_shapes[i][0]))\n",
    "\n",
    "    return w_list, b_list\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "\n",
    "def compile(self, input_shape, output_shape, hidden_shape=[4]):\n",
    "        w_list, b_list = self.init_w_b_matrices(input_shape, output_shape, hidden_shape)\n",
    "        return w_list, b_list\n",
    "    \n",
    "    \n",
    "def feed_forward(self, w_list, b_list, x):\n",
    "        hidden_values = []\n",
    "        for i in range(len(hidden_shape)):\n",
    "            hidden = sigmoid(w_list[i] @ x + b_list[i])\n",
    "            hidden_values.append(hidden)\n",
    "        output_value = w[-1] @ hidden_values[-1]\n",
    "        \n",
    "        return hidden_values\n",
    "    \n",
    "def MSE(y, y_pred):\n",
    "    error = ((y - y_pred) **2).sum()\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
